{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Import\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "# from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "# import librosa\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from IPython.display import display\n",
    "proc = psutil.Process(os.getpid())\n",
    "\n",
    "print('Finish Import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "func = lambda x2, x1: 100.0 * (x2 - x1) / x1\n",
    "\n",
    "def get_data(path, nsamples=16000):\n",
    "    '''Get data from the path and create a pandas dataframe to store it'''\n",
    "    arr = []\n",
    "    valid_label = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "    train_path = path\n",
    "    dir_names = next(os.walk(train_path))[1]\n",
    "        \n",
    "    for label in dir_names:\n",
    "        file_names = next(os.walk(train_path + label))[2]\n",
    "        \n",
    "        # Valid label name\n",
    "        data_size = 600\n",
    "        if label == '_background_noise_':\n",
    "            label_name = 'silence'\n",
    "        elif label not in valid_label:\n",
    "            label_name = 'unknown'\n",
    "        else:\n",
    "            label_name = label\n",
    "            data_size = 1000\n",
    "                \n",
    "        for name in file_names[:data_size]:\n",
    "            path = train_path + label + '/' + name\n",
    "            try:\n",
    "                sample_rate, samples = wavfile.read(path)\n",
    "                \n",
    "                # Scale the length of samples to 16000\n",
    "                if len(samples) < 16000:\n",
    "                    samples = np.pad(samples, (nsamples - sample_len, 0), mode='constant')\n",
    "                else:\n",
    "                    samples = samples[0:nsamples]\n",
    "                    \n",
    "                # Get spectrogram\n",
    "                _, _, spectrogram = log_specgram(samples, sample_rate)\n",
    "                spectrogram = np.reshape(spectrogram, spectrogram.shape + (1, ))\n",
    "                arr.append((path, label_name, sample_rate, samples, spectrogram))\n",
    "                del spectrogram, samples, sample_rate\n",
    "            except:\n",
    "                pass\n",
    "#                 print(path)\n",
    "            del path\n",
    "        print('Finish ', label,)\n",
    "    print('Finish appending array')\n",
    "    return pd.DataFrame(arr, columns=['file_name', 'label', 'sample_rate', 'samples', 'spectrogram'])\n",
    "df = get_data('./train/audio/')\n",
    "\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())\n",
    "display(df.info())\n",
    "# display(df.head(20))\n",
    "# print(df.isnull().sum())\n",
    "gc.collect()\n",
    "# df.to_csv('./resource/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish import model library\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import activations, models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Dense, Input, Dropout, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "print('Finish import model library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(shape):\n",
    "    '''Create a keras functional model'''\n",
    "    \n",
    "    inputlayer = Input(shape=shape)\n",
    "    nclass = 12\n",
    "    \n",
    "    norm_input = BatchNormalization()(inputlayer)\n",
    "    model = Conv2D(16, kernel_size=2, padding='same', activation=activations.relu)(norm_input)\n",
    "    model = Conv2D(16, kernel_size=2, padding='same', activation=activations.relu)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Dropout(rate=0.2)(model)\n",
    "    model = Conv2D(32, kernel_size=3, padding='same', activation=activations.relu)(model)\n",
    "    model = Conv2D(32, kernel_size=3, padding='same', activation=activations.relu)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Dropout(rate=0.2)(model)\n",
    "    model = Conv2D(64, kernel_size=3, padding='same', activation=activations.relu)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Dropout(rate=0.2)(model)\n",
    "    model = Flatten()(model) \n",
    "\n",
    "    dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(model))\n",
    "    dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "    dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "    model = models.Model(inputs=inputlayer, outputs=dense_1)\n",
    "    model.compile(optimizer='adam', loss=tf.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (99, 161, 1)\n",
    "model = get_model(shape)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = df.sample(frac=0.8, replace=False, random_state=42)\n",
    "eval_set = df.loc[set(df.index) - set(train_set.index)]\n",
    "\n",
    "y_train = np.array(train_set.label)\n",
    "y_train = pd.get_dummies(y_train, dtype=bool)\n",
    "x_train = np.array([spec for spec in train_set.spectrogram])\n",
    "\n",
    "y_eval = np.array(eval_set.label)\n",
    "y_eval = pd.get_dummies(y_eval, dtype=bool)\n",
    "x_eval = np.array([spec for spec in eval_set.spectrogram])\n",
    "\n",
    "print(x_train.dtype, x_eval.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x, y, batch_size=16):\n",
    "    # Return a random image from X, y\n",
    "    ylen = len(y)\n",
    "    loopcount = ylen // batch_size\n",
    "    while True:\n",
    "        i = randint(0,loopcount)\n",
    "        yield x[i * batch_size:(i + 1) * batch_size], y[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "# print(len(x_train))\n",
    "batch_size = 10\n",
    "epochs = 12\n",
    "path = './tensorboard/keras_' + str(time())\n",
    "history = model.fit_generator(\n",
    "    generator=batch_generator(x_train, y_train, batch_size),\n",
    "    validation_data=batch_generator(x_eval, y_eval, batch_size),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=y_train.shape[0] // batch_size,\n",
    "    validation_steps=y_eval.shape[0] // batch_size,\n",
    "    callbacks=[TensorBoard(log_dir=path)],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model.save('./model/model_' + str(time()) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './model/model_' + str(time()) + '.h5'\n",
    "model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model1 = load_model('./model/model_1553894437.5339348.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Finish appending array\n"
     ]
    }
   ],
   "source": [
    "def get_predict_data(path):\n",
    "    '''Get prediction data from the path and create a pandas dataframe to store it'''\n",
    "    arr = []\n",
    "    train_path = path\n",
    "    file_names = next(os.walk(train_path))[1]\n",
    "    print(file_names)\n",
    "    for name in file_names:\n",
    "        path = train_path + name\n",
    "        try:\n",
    "            sample_rate, samples = wavfile.read(path)\n",
    "\n",
    "            # Scale the length of samples to 16000\n",
    "            if len(samples) < 16000:\n",
    "                samples = np.pad(samples, (nsamples - sample_len, 0), mode='constant')\n",
    "            else:\n",
    "                samples = samples[0:nsamples]\n",
    "\n",
    "            # Get spectrogram\n",
    "            _, _, spectrogram = log_specgram(samples, sample_rate)\n",
    "            spectrogram = np.reshape(spectrogram, spectrogram.shape + (1, ))\n",
    "            arr.append((path, label_name, sample_rate, samples, spectrogram))\n",
    "            del spectrogram, samples, sample_rate\n",
    "        except:\n",
    "            pass\n",
    "#                 print(path)\n",
    "        del path\n",
    "    print('Finish appending array')\n",
    "    return pd.DataFrame(arr, columns=['file_name', 'label', 'sample_rate', 'samples', 'spectrogram'])\n",
    "df = get_predict_data('./test/audio/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"./test/audio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
